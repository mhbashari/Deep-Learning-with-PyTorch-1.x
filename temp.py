import torch

# https://ruder.io/optimizing-gradient-descent/

# torch.optim.Adadelta
# torch.optim.Adam
# torch.optim.Adagrad
# torch.optim.Adamax
# torch.optim.AdamW
# torch.optim.ASGD
# torch.optim.LBFGS
# torch.optim.RMSprop
# torch.optim.Rprop
# torch.optim.SGD
# torch.optim.SparseAdam
